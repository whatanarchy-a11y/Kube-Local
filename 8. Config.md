# k3s kubeconfig 확인 + Context 개념 정리

> ⚠️ 주의: kubeconfig에는 **클러스터 인증서/토큰/클라이언트 키** 같은 민감 정보가 들어갈 수 있습니다.  
> 화면 공유/깃 업로드/외부 전송 전에 반드시 확인하세요.

---

## 0) kubeconfig란?
`kubeconfig`는 **kubectl(및 Helm 등 Kubernetes 클라이언트)** 이 “어느 클러스터에, 어떤 계정으로, 어떤 네임스페이스로” 접속할지 알려주는 **설정 파일**입니다.

- 보통 위치: `~/.kube/config`
- k3s 기본 kubeconfig: `/etc/rancher/k3s/k3s.yaml`

---

## 1) k3s kubeconfig 위치 확인

### 1-1) 파일 존재/권한 확인
```sh
sudo ls -al /etc/rancher/k3s/k3s.yaml
```

- `-rw-------` 처럼 보이면 일반 사용자로는 읽기 어려울 수 있어요.
- 그래서 아래처럼 `sudo cat` 또는 홈으로 복사가 필요합니다.

---

## 2) kubeconfig 내용 보기

### 2-1) 그대로 출력
```sh
sudo cat /etc/rancher/k3s/k3s.yaml
```

kubeconfig는 YAML 형태이며, 대표적으로 아래 섹션을 가집니다.

- `clusters`: “클러스터 주소(API Server)와 인증서”
- `users`: “사용자/인증 정보(토큰/키 등)”
- `contexts`: “(clusters + users + namespace) 묶음”
- `current-context`: 기본으로 사용할 컨텍스트

---

## 3) kubectl이 실제로 쓰는 kubeconfig 확인 (minify)

### 3-1) 현재 사용 중인 단일 컨텍스트만 보기
```sh
kubectl config view --minify
```

`--minify`는 **현재 컨텍스트 기준으로 필요한 내용만** 보여줘서,
“kubectl이 지금 어디를 바라보는지” 점검하기 좋습니다.

추가로 자주 쓰는 확인 명령:
```sh
kubectl config current-context
kubectl config get-contexts
```

---

## 4) 일반 사용자(ubuntu)로 보기 편하게 복사 (추천)

k3s 기본 kubeconfig(`/etc/rancher/k3s/k3s.yaml`)는 보통 root 권한이 필요합니다.  
`sudo`가 불편하면 홈 디렉토리로 복사해 사용합니다.

### 4-1) 복사 및 권한 설정
```sh
mkdir -p ~/.kube
sudo cp /etc/rancher/k3s/k3s.yaml ~/.kube/config
sudo chown $USER:$USER ~/.kube/config
chmod 600 ~/.kube/config
```

### 4-2) 이제 일반 사용자로 확인/접속
```sh
cat ~/.kube/config
kubectl get nodes
```

---

## 5) Context(컨텍스트) 개념 설명

### 5-1) 컨텍스트는 무엇인가?
**Context = (Cluster + User + Namespace)** 를 한 번에 묶은 “프로필”입니다.

- **Cluster**: 접속 대상 Kubernetes API 서버
- **User**: 어떤 인증/권한으로 접속할지
- **Namespace**: 기본으로 사용할 네임스페이스(선택)

즉 컨텍스트는:
> “이 클러스터에, 이 계정으로, 기본 네임스페이스는 여기로 접속해!”

라는 의미입니다.

---

### 5-2) kubeconfig 내부 구조 예시(간단)
(설명용 예시이며 실제 값은 환경마다 다릅니다)

```yaml
apiVersion: v1
kind: Config

clusters:
- name: default
  cluster:
    server: https://127.0.0.1:6443
    certificate-authority-data: <BASE64...>

users:
- name: default
  user:
    client-certificate-data: <BASE64...>
    client-key-data: <BASE64...>

contexts:
- name: default
  context:
    cluster: default
    user: default
    namespace: default

current-context: default
```

핵심 포인트:
- `contexts[].context.cluster` 가 `clusters[].name` 을 참조
- `contexts[].context.user` 가 `users[].name` 을 참조
- `current-context` 는 “기본으로 쓸 컨텍스트 이름”

---

### 5-3) 컨텍스트 목록 보기 / 전환하기
```sh
kubectl config get-contexts
kubectl config use-context <컨텍스트이름>
kubectl config current-context
```

예:
```sh
kubectl config use-context default
```

---

### 5-4) Namespace 기본값 바꾸기(컨텍스트 단위)
컨텍스트에 기본 namespace를 지정하면 매번 `-n` 옵션을 덜 써도 됩니다.

```sh
kubectl config set-context --current --namespace=demo-hpa
```

확인:
```sh
kubectl config view --minify
kubectl get pods   # 이제 -n demo-hpa 없이도 demo-hpa를 기본으로 봄
```

원복:
```sh
kubectl config set-context --current --namespace=default
```

---

## 6) kubeconfig를 여러 개 쓰는 방법 (자주 쓰는 패턴)

### 6-1) 환경변수 KUBECONFIG
```sh
export KUBECONFIG=/etc/rancher/k3s/k3s.yaml
kubectl get nodes
```

### 6-2) 여러 kubeconfig 합쳐 사용하기
```sh
export KUBECONFIG=~/.kube/config:~/.kube/other-config
kubectl config get-contexts
```

---

## 7) k3s에서 “외부 PC(Windows)에서 kubectl로 붙는” 경우
k3s 기본 kubeconfig는 종종 `server: https://127.0.0.1:6443` 로 되어 있어,
외부 PC에서는 `127.0.0.1`이 **외부 PC 자신**을 의미해 접속이 안 됩니다.

이 경우 kubeconfig의 `server:` 값을 **cp1의 실제 IP**로 바꿔야 합니다(예: `192.168.56.10`).

예:
```sh
sed -i 's#https://127.0.0.1:6443#https://192.168.56.10:6443#g' ~/.kube/config
```

> 단, 외부에서 접근하려면 방화벽/라우팅/포트(6443) 접근 가능 여부도 필요합니다.

---

## 빠른 체크리스트
- kubeconfig 위치: `/etc/rancher/k3s/k3s.yaml`
- 일반 사용자 사용: `~/.kube/config`로 복사 후 권한 600
- 현재 컨텍스트 확인: `kubectl config current-context`
- 컨텍스트 목록: `kubectl config get-contexts`
- 현재 설정 축약 보기: `kubectl config view --minify`
- 네임스페이스 기본값 변경: `kubectl config set-context --current --namespace=<ns>`


```
PS C:\Users\LG> ssh ubuntu@192.168.56.10
ubuntu@192.168.56.10's password:
Welcome to Ubuntu 22.04.5 LTS (GNU/Linux 5.15.0-164-generic x86_64)

 * Documentation:  https://help.ubuntu.com
 * Management:     https://landscape.canonical.com
 * Support:        https://ubuntu.com/pro

This system has been minimized by removing packages and content that are
not required on a system that users do not log into.

To restore this content, you can run the 'unminimize' command.
New release '24.04.3 LTS' available.
Run 'do-release-upgrade' to upgrade to it.

Last login: Sat Jan 10 04:54:13 2026 from 192.168.56.1
ubuntu@cp1:~$ ls -al
total 44
drwxr-x--- 6 ubuntu ubuntu 4096 Jan  4 02:56 .
drwxr-xr-x 3 root   root   4096 Jan  1 06:28 ..
-rw------- 1 ubuntu ubuntu 6450 Jan 10 05:08 .bash_history
-rw-r--r-- 1 ubuntu ubuntu    0 Jan  6  2022 .bash_logout
-rw-r--r-- 1 ubuntu ubuntu   44 Jan  4 02:58 .bashrc
drwx------ 3 ubuntu ubuntu 4096 Jan  4 02:56 .cache
drwxrwxr-x 3 ubuntu ubuntu 4096 Jan  4 02:56 .config
drwxr-x--- 3 ubuntu ubuntu 4096 Jan  3 02:34 .kube
-rw-r--r-- 1 ubuntu ubuntu    0 Jan  6  2022 .profile
drwx------ 2 ubuntu ubuntu 4096 Jan  4 03:01 .ssh
-rw-r--r-- 1 ubuntu ubuntu    0 Jan  1 07:59 .sudo_as_admin_successful
-rw------- 1 ubuntu ubuntu  840 Jan  3 02:44 .viminfo
-rw-rw-r-- 1 ubuntu ubuntu  283 Jan  3 02:44 ing-whoami.yaml
ubuntu@cp1:~$ cd .kube
ubuntu@cp1:~/.kube$ ls -al
total 12
drwxr-x--- 3 ubuntu ubuntu 4096 Jan  3 02:34 .
drwxr-x--- 6 ubuntu ubuntu 4096 Jan  4 02:56 ..
drwxr-x--- 4 ubuntu ubuntu 4096 Jan  3 02:34 cache
ubuntu@cp1:~/.kube$ cd config
-bash: cd: config: No such file or directory
ubuntu@cp1:~/.kube$ ls -al
total 12
drwxr-x--- 3 ubuntu ubuntu 4096 Jan  3 02:34 .
drwxr-x--- 6 ubuntu ubuntu 4096 Jan  4 02:56 ..
drwxr-x--- 4 ubuntu ubuntu 4096 Jan  3 02:34 cache
ubuntu@cp1:~/.kube$ echo $KUBECONFIG

ubuntu@cp1:~/.kube$ ls -la ~/.kube
total 12
drwxr-x--- 3 ubuntu ubuntu 4096 Jan  3 02:34 .
drwxr-x--- 6 ubuntu ubuntu 4096 Jan  4 02:56 ..
drwxr-x--- 4 ubuntu ubuntu 4096 Jan  3 02:34 cache
ubuntu@cp1:~/.kube$ # 1. 환경변수 확인 (가장 먼저 확인하세요!)
echo $KUBECONFIG

# 2. 기본 위치에 정말 없는지 다시 한 번 확인
ls -la ~/.kube

# 3. kubeadm으로 만든 클러스터라면 대개 여기에 있음
sudo ls -la /etc/kubernetes/admin.conf
# 있으면 → 보통 이 파일을 사용자 홈으로 복사해서 사용
sudo cp /etc/kubernetes/admin.conf ~/.kube/config
sudo chown ubuntu:ubuntu ~/.kube/config
chmod 600 ~/.kube/config

# 4. 현재 디렉토리에 혹시 있나?
ls -la ~/config   2>/dev/null || ls -la ./config 2>/dev/null

# 5. kubectl이 실제로 어떤 설정을 보고 있는지 확인
kubectl config view --minify
# → 에러가 나면 kubeconfig를 못 찾고 있다는 뜻

total 12
drwxr-x--- 3 ubuntu ubuntu 4096 Jan  3 02:34 .
drwxr-x--- 6 ubuntu ubuntu 4096 Jan  4 02:56 ..
drwxr-x--- 4 ubuntu ubuntu 4096 Jan  3 02:34 cache
[sudo] password for ubuntu:
ls: cannot access '/etc/kubernetes/admin.conf': No such file or directory
cp: cannot stat '/etc/kubernetes/admin.conf': No such file or directory
chown: cannot access '/home/ubuntu/.kube/config': No such file or directory
chmod: cannot access '/home/ubuntu/.kube/config': No such file or directory
apiVersion: v1
clusters:
- cluster:
    certificate-authority-data: DATA+OMITTED
    server: https://127.0.0.1:6443
  name: default
contexts:
- context:
    cluster: default
    user: default
  name: default
current-context: default
kind: Config
users:
- name: default
  user:
    client-certificate-data: DATA+OMITTED
    client-key-data: DATA+OMITTED
ubuntu@cp1:~/.kube$ # 1. 파일 내용 확인 (root 권한 필요)
sudo cat /etc/rancher/k3s/k3s.yaml

# 2. 일반 사용자(ubuntu)가 kubectl로 바로 사용할 수 있게 복사 및 권한 변경
mkdir -p ~/.kube
sudo cp /etc/rancher/k3s/k3s.yaml ~/.kube/config
sudo chown ubuntu:ubuntu ~/.kube/config
chmod 600 ~/.kube/config

# 3. 잘 되는지 확인
kubectl get nodes
kubectl get pods -A
apiVersion: v1
clusters:
- cluster:
    certificate-authority-data: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUJkakNDQVIyZ0F3SUJBZ0lCQURBS0JnZ3Foa2pPUFFRREFqQWpNU0V3SHdZRFZRUUREQmhyTTNNdGMyVnkKZG1WeUxXTmhRREUzTmpjME1EWXhNakl3SGhjTk1qWXdNVEF6TURJd09EUXlXaGNOTXpZd01UQXhNREl3T0RReQpXakFqTVNFd0h3WURWUVFEREJock0zTXRjMlZ5ZG1WeUxXTmhRREUzTmpjME1EWXhNakl3V1RBVEJnY3Foa2pPClBRSUJCZ2dxaGtqT1BRTUJCd05DQUFRd0tIektIWE0wS3Q3bEQ3N1ZvRGsveDB5cXlGdmlzcXoxQWVXRWNrQWoKcjB5U2tXTTU4T3JTazdTZ1N1M0JCdTVKaTgxd3VnSjFST2drVFYxYkJmN0VvMEl3UURBT0JnTlZIUThCQWY4RQpCQU1DQXFRd0R3WURWUjBUQVFIL0JBVXdBd0VCL3pBZEJnTlZIUTRFRmdRVWhyaGVMNWlFQzFTTjVad1dPTmVRCit1VUdWbk13Q2dZSUtvWkl6ajBFQXdJRFJ3QXdSQUlnWEh3RFpMZ3dwVmpvdGx3bXY0YllJVVN0ZDJVVGJ0eEMKejg1ejdnWmxUaU1DSUN2eUhOVFRJYWIyZ3hQUFJsMTNjSG01M2dIVWdpa0xoQUs1VDJNMHpQWWMKLS0tLS1FTkQgQ0VSVElGSUNBVEUtLS0tLQo=
    server: https://127.0.0.1:6443
  name: default
contexts:
- context:
    cluster: default
    user: default
  name: default
current-context: default
kind: Config
users:
- name: default
  user:
    client-certificate-data: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUJrVENDQVRlZ0F3SUJBZ0lJUjlSMGh2cFRGaHN3Q2dZSUtvWkl6ajBFQXdJd0l6RWhNQjhHQTFVRUF3d1kKYXpOekxXTnNhV1Z1ZEMxallVQXhOelkzTkRBMk1USXlNQjRYRFRJMk1ERXdNekF5TURnME1sb1hEVEkzTURFdwpNekF5TURnME1sb3dNREVYTUJVR0ExVUVDaE1PYzNsemRHVnRPbTFoYzNSbGNuTXhGVEFUQmdOVkJBTVRESE41CmMzUmxiVHBoWkcxcGJqQlpNQk1HQnlxR1NNNDlBZ0VHQ0NxR1NNNDlBd0VIQTBJQUJGWXlFWjdYdGZqTkE5d2wKYW1PMlUxRkNFWkcxaXl5NytkcUxjN1YzWGwrS2x3K3Z2UzNaaVZWSzIvU3pIOXlKdUdlcXFVTGxHM3F4Q2VDdwpjUjMyZ0Z5alNEQkdNQTRHQTFVZER3RUIvd1FFQXdJRm9EQVRCZ05WSFNVRUREQUtCZ2dyQmdFRkJRY0RBakFmCkJnTlZIU01FR0RBV2dCVDRFVklyMTg0eUJFaDU4OVVQNlpOUXRwWkVPakFLQmdncWhrak9QUVFEQWdOSUFEQkYKQWlFQTA2T2JxeVNFUjVacGJCeE1qcTNtd1JIK1F2VVVLaENzNE5tK0M0RnFWd1lDSUh2TnpGYitraFo3c0p4SgozcHBiTGZQdE1YcjNJaHhLQmgxd1NmTUxoakNiCi0tLS0tRU5EIENFUlRJRklDQVRFLS0tLS0KLS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUJkakNDQVIyZ0F3SUJBZ0lCQURBS0JnZ3Foa2pPUFFRREFqQWpNU0V3SHdZRFZRUUREQmhyTTNNdFkyeHAKWlc1MExXTmhRREUzTmpjME1EWXhNakl3SGhjTk1qWXdNVEF6TURJd09EUXlXaGNOTXpZd01UQXhNREl3T0RReQpXakFqTVNFd0h3WURWUVFEREJock0zTXRZMnhwWlc1MExXTmhRREUzTmpjME1EWXhNakl3V1RBVEJnY3Foa2pPClBRSUJCZ2dxaGtqT1BRTUJCd05DQUFUdlEwYWdZVzBkb0dPVkFXSFpZbFpidHNKTEFQVmFaaDF2ZTY3YlB6VXMKTTU4WGQycjkzdWpza0lVOXVkZ0FkWXR6SlBodmZ6eWNScFBiSExBaHNGWHdvMEl3UURBT0JnTlZIUThCQWY4RQpCQU1DQXFRd0R3WURWUjBUQVFIL0JBVXdBd0VCL3pBZEJnTlZIUTRFRmdRVStCRlNLOWZPTWdSSWVmUFZEK21UClVMYVdSRG93Q2dZSUtvWkl6ajBFQXdJRFJ3QXdSQUlnT1cweFNheWd3YTFlTjlpWWhOS1dReHBEbDlBbldZM3AKbjlkWEx1cDdCMndDSUNoM0dqNUFmT3hBQ0Fya0VVOFljeVI0WkVFTDZEV1JpQ0duakxyVW1BaXMKLS0tLS1FTkQgQ0VSVElGSUNBVEUtLS0tLQo=
    client-key-data: LS0tLS1CRUdJTiBFQyBQUklWQVRFIEtFWS0tLS0tCk1IY0NBUUVFSU5MZWFYLzdhdExXeDc0T2hlcDNpNkFKNXJReU00em5HRkNmOGwvVVJIQVZvQW9HQ0NxR1NNNDkKQXdFSG9VUURRZ0FFVmpJUm50ZTErTTBEM0NWcVk3WlRVVUlSa2JXTExMdjUyb3R6dFhkZVg0cVhENis5TGRtSgpWVXJiOUxNZjNJbTRaNnFwUXVVYmVyRUo0TEJ4SGZhQVhBPT0KLS0tLS1FTkQgRUMgUFJJVkFURSBLRVktLS0tLQo=
NAME   STATUS     ROLES           AGE    VERSION
cp1    Ready      control-plane   7d6h   v1.34.3+k3s1
w1     NotReady   <none>          7d6h   v1.34.3+k3s1
w2     NotReady   <none>          7d6h   v1.34.3+k3s1
NAMESPACE              NAME                                                    READY   STATUS             RESTARTS   AGE
default                nginx-66686b6766-6jkr4                                  1/1     Terminating        0          7d6h
default                nginx-66686b6766-7x2d9                                  1/1     Running            0          7d6h
default                nginx-66686b6766-d4xtt                                  1/1     Running            0          3h23m
default                nginx-66686b6766-frj65                                  1/1     Running            0          4m58s
default                nginx-66686b6766-v4fsg                                  1/1     Terminating        0          7d6h
default                whoami-b85fc56b4-fk6p4                                  1/1     Running            0          4m58s
default                whoami-b85fc56b4-z2vrp                                  1/1     Terminating        0          7d5h
demo-hpa               hey                                                     0/1     ImagePullBackOff   0          3h51m
demo-hpa               nginx-c7d5945f8-7c769                                   1/1     Running            0          4m57s
demo-hpa               nginx-c7d5945f8-hh5wg                                   1/1     Running            0          3h56m
demo-hpa               nginx-c7d5945f8-xv6lp                                   1/1     Terminating        0          3h45m
kube-system            coredns-7f496c8d7d-bjdjf                                1/1     Running            0          7d6h
kube-system            helm-install-traefik-crd-rjtd8                          0/1     Completed          0          7d6h
kube-system            helm-install-traefik-zxz5t                              0/1     Completed          1          7d6h
kube-system            local-path-provisioner-578895bd58-lp56l                 1/1     Running            0          7d6h
kube-system            metrics-server-7b9c9c4b9c-pw2fz                         1/1     Running            0          7d6h
kube-system            svclb-traefik-e918a231-dfppl                            2/2     Running            0          7d6h
kube-system            svclb-traefik-e918a231-mtwlj                            2/2     Running            0          7d6h
kube-system            svclb-traefik-e918a231-vc6kg                            2/2     Running            0          7d6h
kube-system            traefik-6f5f87584-lq22d                                 1/1     Running            0          7d6h
kubernetes-dashboard   kubernetes-dashboard-api-6d5bb95b6c-nzjvg               1/1     Terminating        0          6d5h
kubernetes-dashboard   kubernetes-dashboard-api-6d5bb95b6c-w2xtc               1/1     Running            0          3h23m
kubernetes-dashboard   kubernetes-dashboard-auth-6f886b65cf-bp4hc              1/1     Terminating        0          6d5h
kubernetes-dashboard   kubernetes-dashboard-auth-6f886b65cf-txz84              1/1     Running            0          3h23m
kubernetes-dashboard   kubernetes-dashboard-kong-9849c64bd-dfjx2               1/1     Running            0          3h23m
kubernetes-dashboard   kubernetes-dashboard-kong-9849c64bd-hrcx2               1/1     Terminating        0          6d5h
kubernetes-dashboard   kubernetes-dashboard-metrics-scraper-7685fd8b77-95wkp   1/1     Terminating        0          6d5h
kubernetes-dashboard   kubernetes-dashboard-metrics-scraper-7685fd8b77-vnm6p   1/1     Running            0          4m58s
kubernetes-dashboard   kubernetes-dashboard-web-5c9f966b98-gd5c6               1/1     Terminating        0          6d5h
kubernetes-dashboard   kubernetes-dashboard-web-5c9f966b98-hjw6n               1/1     Running            0          4m58s
ubuntu@cp1:~/.kube$ ls -al
total 16
drwxr-x--- 3 ubuntu ubuntu 4096 Jan 10 08:37 .
drwxr-x--- 6 ubuntu ubuntu 4096 Jan  4 02:56 ..
drwxr-x--- 4 ubuntu ubuntu 4096 Jan  3 02:34 cache
-rw------- 1 ubuntu ubuntu 2937 Jan 10 08:37 config

ubuntu@cp1:~/.kube$ kubectl config get-contexts
CURRENT   NAME      CLUSTER   AUTHINFO   NAMESPACE
*         default   default   default
ubuntu@cp1:~/.kube$ sudo ls -al /etc/rancher/k3s/k3s.yaml
-rw-r--r-- 1 root root 2937 Jan  3 02:08 /etc/rancher/k3s/k3s.yaml
ubuntu@cp1:~/.kube$ kubectl config view --minify
apiVersion: v1
clusters:
- cluster:
    certificate-authority-data: DATA+OMITTED
    server: https://127.0.0.1:6443
  name: default
contexts:
- context:
    cluster: default
    user: default
  name: default
current-context: default
kind: Config
users:
- name: default
  user:
    client-certificate-data: DATA+OMITTED
    client-key-data: DATA+OMITTED
ubuntu@cp1:~/.kube$
```